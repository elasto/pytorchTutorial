{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = torch.tensor(1.0)\n", "y = torch.tensor(2.0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is the parameter we want to optimize -> requires_grad=True"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w = torch.tensor(1.0, requires_grad=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["forward pass to compute loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_predicted = w * x\n", "loss = (y_predicted - y)**2\n", "print(loss)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["backward pass to compute gradient dLoss/dw"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loss.backward()\n", "print(w.grad)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["update weights<br>\n", "next forward and backward pass..."]}, {"cell_type": "markdown", "metadata": {}, "source": ["continue optimizing:<br>\n", "update weights, this operation should not be part of the computational graph"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with torch.no_grad():\n", "    w -= 0.01 * w.grad\n", "# don't forget to zero the gradients\n", "w.grad.zero_()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["next forward and backward pass..."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}