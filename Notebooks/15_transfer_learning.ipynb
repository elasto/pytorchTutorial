{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "from torch.optim import lr_scheduler\n", "import numpy as np\n", "import torchvision\n", "from torchvision import datasets, models, transforms\n", "import matplotlib.pyplot as plt\n", "import time\n", "import os\n", "import copy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mean = np.array([0.5, 0.5, 0.5])\n", "std = np.array([0.25, 0.25, 0.25])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_transforms = {\n", "    'train': transforms.Compose([\n", "        transforms.RandomResizedCrop(224),\n", "        transforms.RandomHorizontalFlip(),\n", "        transforms.ToTensor(),\n", "        transforms.Normalize(mean, std)\n", "    ]),\n", "    'val': transforms.Compose([\n", "        transforms.Resize(256),\n", "        transforms.CenterCrop(224),\n", "        transforms.ToTensor(),\n", "        transforms.Normalize(mean, std)\n", "    ]),\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_dir = 'data/hymenoptera_data'\n", "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n", "                                          data_transforms[x])\n", "                  for x in ['train', 'val']}\n", "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n", "                                             shuffle=True, num_workers=0)\n", "              for x in ['train', 'val']}\n", "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n", "class_names = image_datasets['train'].classes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n", "print(class_names)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def imshow(inp, title):\n", "    \"\"\"Imshow for Tensor.\"\"\"\n", "    inp = inp.numpy().transpose((1, 2, 0))\n", "    inp = std * inp + mean\n", "    inp = np.clip(inp, 0, 1)\n", "    plt.imshow(inp)\n", "    plt.title(title)\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Get a batch of training data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["inputs, classes = next(iter(dataloaders['train']))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make a grid from batch"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["out = torchvision.utils.make_grid(inputs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["imshow(out, title=[class_names[x] for x in classes])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n", "    since = time.time()\n", "    best_model_wts = copy.deepcopy(model.state_dict())\n", "    best_acc = 0.0\n", "    for epoch in range(num_epochs):\n", "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n", "        print('-' * 10)\n\n", "        # Each epoch has a training and validation phase\n", "        for phase in ['train', 'val']:\n", "            if phase == 'train':\n", "                model.train()  # Set model to training mode\n", "            else:\n", "                model.eval()   # Set model to evaluate mode\n", "            running_loss = 0.0\n", "            running_corrects = 0\n\n", "            # Iterate over data.\n", "            for inputs, labels in dataloaders[phase]:\n", "                inputs = inputs.to(device)\n", "                labels = labels.to(device)\n", "                # forward\n", "                # track history if only in train\n", "                with torch.set_grad_enabled(phase == 'train'):\n", "                    outputs = model(inputs)\n", "                    _, preds = torch.max(outputs, 1)\n", "                    loss = criterion(outputs, labels)\n", "                    # backward + optimize only if in training phase\n", "                    if phase == 'train':\n", "                        optimizer.zero_grad()\n", "                        loss.backward()\n", "                        optimizer.step()\n", "                # statistics\n", "                running_loss += loss.item() * inputs.size(0)\n", "                running_corrects += torch.sum(preds == labels.data)\n", "            if phase == 'train':\n", "                scheduler.step()\n", "            epoch_loss = running_loss / dataset_sizes[phase]\n", "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n", "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n", "                phase, epoch_loss, epoch_acc))\n\n", "            # deep copy the model\n", "            if phase == 'val' and epoch_acc > best_acc:\n", "                best_acc = epoch_acc\n", "                best_model_wts = copy.deepcopy(model.state_dict())\n", "        print()\n", "    time_elapsed = time.time() - since\n", "    print('Training complete in {:.0f}m {:.0f}s'.format(\n", "        time_elapsed // 60, time_elapsed % 60))\n", "    print('Best val Acc: {:4f}'.format(best_acc))\n\n", "    # load best model weights\n", "    model.load_state_dict(best_model_wts)\n", "    return model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Finetuning the convnet ####<br>\n", "Load a pretrained model and reset final fully connected layer."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = models.resnet18(pretrained=True)\n", "num_ftrs = model.fc.in_features\n", "# Here the size of each output sample is set to 2.\n", "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n", "model.fc = nn.Linear(num_ftrs, 2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = model.to(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["criterion = nn.CrossEntropyLoss()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Observe that all parameters are being optimized"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["optimizer = optim.SGD(model.parameters(), lr=0.001)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["StepLR Decays the learning rate of each parameter group by gamma every step_size epochs<br>\n", "Decay LR by a factor of 0.1 every 7 epochs<br>\n", "Learning rate scheduling should be applied after optimizer\u00e2\u20ac\u2122s update<br>\n", "e.g., you should write your code this way:<br>\n", "for epoch in range(100):<br>\n", "    train(...)<br>\n", "    validate(...)<br>\n", "    scheduler.step()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=25)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## ConvNet as fixed feature extractor ####<br>\n", "Here, we need to freeze all the network except the final layer.<br>\n", "We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_conv = torchvision.models.resnet18(pretrained=True)\n", "for param in model_conv.parameters():\n", "    param.requires_grad = False"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Parameters of newly constructed modules have requires_grad=True by default"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_ftrs = model_conv.fc.in_features\n", "model_conv.fc = nn.Linear(num_ftrs, 2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_conv = model_conv.to(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["criterion = nn.CrossEntropyLoss()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Observe that only parameters of final layer are being optimized as<br>\n", "opposed to before."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Decay LR by a factor of 0.1 every 7 epochs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_conv = train_model(model_conv, criterion, optimizer_conv,\n", "                         exp_lr_scheduler, num_epochs=25)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}