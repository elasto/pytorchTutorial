{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["output = w*x + b<br>\n", "output = activation_function(output)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = torch.tensor([-1.0, 1.0, 2.0, 3.0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["sofmax"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output = torch.softmax(x, dim=0)\n", "print(output)\n", "sm = nn.Softmax(dim=0)\n", "output = sm(x)\n", "print(output)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["sigmoid "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output = torch.sigmoid(x)\n", "print(output)\n", "s = nn.Sigmoid()\n", "output = s(x)\n", "print(output)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["anh"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output = torch.tanh(x)\n", "print(output)\n", "t = nn.Tanh()\n", "output = t(x)\n", "print(output)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["relu"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output = torch.relu(x)\n", "print(output)\n", "relu = nn.ReLU()\n", "output = relu(x)\n", "print(output)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["leaky relu"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output = F.leaky_relu(x)\n", "print(output)\n", "lrelu = nn.LeakyReLU()\n", "output = lrelu(x)\n", "print(output)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["n.ReLU() creates an nn.Module which you can add e.g. to an nn.Sequential model.<br>\n", "orch.relu on the other side is just the functional API call to the relu function,<br>\n", "o that you can add it e.g. in your forward method yourself."]}, {"cell_type": "markdown", "metadata": {}, "source": ["option 1 (create nn modules)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NeuralNet(nn.Module):\n", "    def __init__(self, input_size, hidden_size):\n", "        super(NeuralNet, self).__init__()\n", "        self.linear1 = nn.Linear(input_size, hidden_size)\n", "        self.relu = nn.ReLU()\n", "        self.linear2 = nn.Linear(hidden_size, 1)\n", "        self.sigmoid = nn.Sigmoid()\n", "    \n", "    def forward(self, x):\n", "        out = self.linear1(x)\n", "        out = self.relu(out)\n", "        out = self.linear2(out)\n", "        out = self.sigmoid(out)\n", "        return out"]}, {"cell_type": "markdown", "metadata": {}, "source": ["option 2 (use activation functions directly in forward pass)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NeuralNet(nn.Module):\n", "    def __init__(self, input_size, hidden_size):\n", "        super(NeuralNet, self).__init__()\n", "        self.linear1 = nn.Linear(input_size, hidden_size)\n", "        self.linear2 = nn.Linear(hidden_size, 1)\n", "    \n", "    def forward(self, x):\n", "        out = torch.relu(self.linear1(x))\n", "        out = torch.sigmoid(self.linear2(out))\n", "        return out"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}