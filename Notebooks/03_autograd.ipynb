{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "# The autograd package provides automatic differentiation \n", "# for all operations on Tensors"]}, {"cell_type": "markdown", "metadata": {}, "source": ["requires_grad = True -> tracks all operations on the tensor. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = torch.randn(3, requires_grad=True)\n", "y = x + 2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["y was created as a result of an operation, so it has a grad_fn attribute.<br>\n", "grad_fn: references a Function that has created the Tensor"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(x) # created by the user -> grad_fn is None\n", "print(y)\n", "print(y.grad_fn)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Do more operations on y"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["z = y * y * 3\n", "print(z)\n", "z = z.mean()\n", "print(z)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's compute the gradients with backpropagation<br>\n", "When we finish our computation we can call .backward() and have all the gradients computed automatically.<br>\n", "The gradient for this tensor will be accumulated into .grad attribute.<br>\n", "It is the partial derivate of the function w.r.t. the tensor"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["z.backward()\n", "print(x.grad) # dz/dx"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Generally speaking, torch.autograd is an engine for computing vector-Jacobian product<br>\n", "It computes partial derivates while applying the chain rule"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------<br>\n", "Model with non-scalar output:<br>\n", "If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() <br>\n", "specify a gradient argument that is a tensor of matching shape.<br>\n", "needed for vector-Jacobian product"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = torch.randn(3, requires_grad=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = x * 2\n", "for _ in range(10):\n", "    y = y * 2"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(y)\n", "print(y.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n", "y.backward(v)\n", "print(x.grad)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------<br>\n", "Stop a tensor from tracking history:<br>\n", "For example during our training loop when we want to update our weights<br>\n", "then this update operation should not be part of the gradient computation<br>\n", "- x.requires_grad_(False)<br>\n", "- x.detach()<br>\n", "- wrap in 'with torch.no_grad():'"]}, {"cell_type": "markdown", "metadata": {}, "source": [".requires_grad_(...) changes an existing flag in-place."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["a = torch.randn(2, 2)\n", "print(a.requires_grad)\n", "b = ((a * 3) / (a - 1))\n", "print(b.grad_fn)\n", "a.requires_grad_(True)\n", "print(a.requires_grad)\n", "b = (a * a).sum()\n", "print(b.grad_fn)"]}, {"cell_type": "markdown", "metadata": {}, "source": [".detach(): get a new Tensor with the same content but no gradient computation:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["a = torch.randn(2, 2, requires_grad=True)\n", "print(a.requires_grad)\n", "b = a.detach()\n", "print(b.requires_grad)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["wrap in 'with torch.no_grad():'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["a = torch.randn(2, 2, requires_grad=True)\n", "print(a.requires_grad)\n", "with torch.no_grad():\n", "    print((x ** 2).requires_grad)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------<br>\n", "backward() accumulates the gradient for this tensor into .grad attribute.<br>\n", "!!! We need to be careful during optimization !!!<br>\n", "Use .zero_() to empty the gradients before a new optimization step!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["weights = torch.ones(4, requires_grad=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for epoch in range(3):\n", "    # just a dummy example\n", "    model_output = (weights*3).sum()\n", "    model_output.backward()\n", "    \n", "    print(weights.grad)\n\n", "    # optimize model, i.e. adjust weights...\n", "    with torch.no_grad():\n", "        weights -= 0.1 * weights.grad\n\n", "    # this is important! It affects the final weights & output\n", "    weights.grad.zero_()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(weights)\n", "print(model_output)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Optimizer has zero_grad() method<br>\n", "optimizer = torch.optim.SGD([weights], lr=0.1)<br>\n", "During training:<br>\n", "optimizer.step()<br>\n", "optimizer.zero_grad()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}